# -*- coding: utf-8 -*-
"""Feed Forward VQGAN CLIP - Using a pre-trained model .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1N8vvdhkvLaMefTIW_WYuJa-FflqyBnHr

# Feed Forward VQGAN_CLIP

Feed forward VQGAN-CLIP model, where the goal is to eliminate the need for optimizing the latent space of VQGAN for each input prompt. This is done by training a model that takes as input a text prompt, and returns as an output the VQGAN latent space, which is then transformed into an RGB image. The model is trained on a dataset of text prompts and can be used on unseen text prompts. The loss function is minimizing the distance between the CLIP generated image features and the CLIP input text features. Additionally, a diversity loss can be used to make increase the diversity of the generated images given the same prompt.

This notebooks shows how to use a pre-trained model for generating images.
"""

import os, sys
import torch

!git clone https://github.com/mehdidc/feed_forward_vqgan_clip

cd feed_forward_vqgan_clip

!pip install -r requirements.txt

device = "cuda" if torch.cuda.is_available() else "cpu"

# Optional, only for super-resolution
!wget  https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth 
!wget  https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth

# Commented out IPython magic to ensure Python compatibility.
# Optional, only for super-resolution
!git clone https://github.com/sberbank-ai/Real-ESRGAN
# %cd Real-ESRGAN
!pip install -r requirements.txt
# %cd ..
scale = 4 # either 2 or 4
sys.path.append("Real-ESRGAN")
from realesrgan import RealESRGAN
realesrgan = RealESRGAN(device, scale=scale)
realesrgan.load_weights(f'RealESRGAN_x{scale}plus.pth')

#check available models at https://github.com/mehdidc/feed_forward_vqgan_clip/releases
from download_weights import model_url, download
download("https://github.com/mehdidc/feed_forward_vqgan_clip/releases/download/0.1/vqgan_imagenet_f16_16384.yaml")
download("https://github.com/mehdidc/feed_forward_vqgan_clip/releases/download/0.1/vqgan_imagenet_f16_16384.ckpt")

# Model selection
import ipywidgets as widgets
dropdown = widgets.Dropdown(
    options=model_url.keys(),
    value='cc12m_32x1024_mlp_mixer_clip_ViTB32_256x256_v0.3.th',
    description='Model:',
    disabled=False,
    layout={'width': 'max-content'},
)
print("Please select the model:")
dropdown

# Download the selected model
model_path = dropdown.value
if 'cloob' in model_path:
  download("https://ml.jku.at/research/CLOOB/downloads/checkpoints/cloob_rn50_yfcc_epoch_28.pt")
print("Selected model: ", model_path)
if not os.path.exists(model_path):
  print("Downloading", model_path)
  url = model_url[model_path]
  !wget $url --output-document=$model_path

"""# Load model

---


"""

from IPython.display import Image
import torch
import clip
from main import load_vqgan_model, CLIP_DIM, clamp_with_grad, synth, load_clip_model
import torchvision
net = torch.load(model_path, map_location="cpu").to(device)
config = net.config
vqgan_config = config.vqgan_config 
vqgan_checkpoint = config.vqgan_checkpoint
clip_model = config.clip_model
clip_dim = CLIP_DIM[clip_model]
if config.get("clip_model_path"):
  assert os.path.exists(config.clip_model_path)
perceptor = load_clip_model(clip_model, path=config.get("clip_model_path")).eval().requires_grad_(False).to(device)
model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)
z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]
z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]

"""# Generation of images from text"""

# Please provide a single or a list of text prompts.
# Each text prompt of the list is used to generate an independent image.
texts = [
   #"Picture of a futuristic snowy city during the night, the tree is lit with a lantern.",
   #"Castle made of chocolate",
   #"Mushroom with strange colors", 
   #"a professional high quality illustration of a giraffe spider chimera. a giraffe imitating a spider. a giraffe made of spider.",
   #"bedroom from 1700",
   "tetris",
]
toks = clip.tokenize(texts, truncate=True)
H = perceptor.encode_text(toks.to(device)).float()
with torch.no_grad():
    z = net(H)
    z = clamp_with_grad(z, z_min.min(),
     z_max.max())
    xr = synth(model, z)
grid = torchvision.utils.make_grid(xr.cpu(), nrow=len(xr))
pil_image = torchvision.transforms.functional.to_pil_image(grid)
pil_image

# Commented out IPython magic to ensure Python compatibility.
# %%time
# sr_image = realesrgan.predict(pil_image)
# display(sr_image)

"""# Video Generation"""

from base64 import b64encode
from IPython.display import HTML
nb_interm = 32 # nb of intermediate images between each successive text prompts
bs = 8 # reduce bs (batch size) if memory error
text_sequence = [
  "plant root",
  'small flower',
  'tulip',
  'garden full of tulips',
]

toks = clip.tokenize(text_sequence, truncate=True)
alpha = torch.linspace(0,1,nb_interm).view(-1,1).to(device)
feats = perceptor.encode_text(toks.to(device)).float()

H_list = []
for i in range(len(text_sequence)-1):
  Hi = feats[i:i+1] * (1-alpha) + feats[i+1:i+2] * alpha
  H_list.append(Hi)
H = torch.cat(H_list)
xr_list = []
with torch.no_grad():
  for i in range(0, len(H), bs):
    z = net(H[i:i+bs])
    z = clamp_with_grad(z, z_min.min(), z_max.max())
    xr = synth(model, z)
    xr_list.append(xr.cpu())
xr = torch.cat(xr_list)
grid = torchvision.utils.make_grid(xr.cpu(), nrow=len(xr))
!rm -f *.png *.mp4
out_path = "gen.png"
torchvision.transforms.functional.to_pil_image(grid).save(out_path)
for i, img in enumerate(xr):
  torchvision.transforms.functional.to_pil_image(img).save(f"image_{i:05d}.png")
!ffmpeg -framerate 15 -pattern_type glob -i 'image*.png'  -c:v libx264 -r 30 -pix_fmt yuv420p video.mp4 1>&2 2>/dev/null
# Show video
mp4 = open("video.mp4",'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()
HTML("""
<video width=256 height=256 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)

Image("gen.png")